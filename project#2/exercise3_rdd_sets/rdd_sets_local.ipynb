{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is worth 10% of the grade of project 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports many of the operations we have in mathematical sets, such as union and intersection, even when the RDDs themselves are not properly sets. It is important to note that these operations require that the RDDs being operated on are of the same type.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set operations are quite straightforward to understand as it work as expected. The only consideration comes from the fact that RDDs are not real sets, and therefore operations such as the union of RDDs doesn't remove duplicates. In this notebook we will have a brief look at `subtract`, `distinct`, and `cartesian`.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the reduced dataset (1 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a *Gzip* file in the local directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "data_file = os.getcwd() + \"/../kddcup.data_1_percent.gz\"\n",
    "# TODO: read the textFile from 'data_file' into 'raw_data'. Hint: use sc.textFile(<local path>) \n",
    "with gzip.open(data_file, \"rt\") as f:\n",
    "    raw_data = [row.strip() for row in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting attack interactions using `subtract`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, imagine we already have our RDD with non attack (normal) interactions from some previous analysis.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = [row for row in raw_data if \"normal.\" in row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can obtain attack interactions by subtracting normal ones from the original unfiltered RDD as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "The correct answer should be a RDD object\n"
     ]
    }
   ],
   "source": [
    "# TODO: generate a RDD `attack_raw_data` as `raw_data` subtraced by `normal_raw_data`\n",
    "attack_raw_data = [row for row in raw_data if row not in normal_raw_data]\n",
    "print(type(attack_raw_data))\n",
    "print(\"The correct answer should be a RDD object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some counts to check our results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All count in 0.0 secs\n",
      "Len raw_data: 49402\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# count all\n",
    "t0 = time()\n",
    "raw_data_count = len(raw_data)\n",
    "tt = time() - t0\n",
    "print(\"All count in {} secs\".format(round(tt,3)))\n",
    "print (\"Len raw_data: {}\".format(raw_data_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count in 0.0 secs\n"
     ]
    }
   ],
   "source": [
    "# count normal\n",
    "t0 = time()\n",
    "normal_raw_data_count = len(normal_raw_data)\n",
    "tt = time() - t0\n",
    "print(\"Normal count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack count in 0.0 secs\n"
     ]
    }
   ],
   "source": [
    "# count attacks\n",
    "t0 = time()\n",
    "attack_raw_data_count = len(attack_raw_data)\n",
    "tt = time() - t0\n",
    "print(\"Attack count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9641 normal interactions and 39761 attacks, from a total of 49402 interactions\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} normal interactions and {} attacks, from a total of {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have two RDDs, one with normal interactions and another one with attacks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol and service combinations using `cartesian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the Cartesian product between two RDDs by using the `cartesian` transformation. It returns all possible pairs of elements between two RDDs. In our case we will use it to generate all the possible combinations between service and protocol in our network interactions.  \n",
    "\n",
    "First of all we need to isolate each collection of values in two separate RDDs. For that we will use `distinct` on the CSV-parsed dataset. From the [dataset description](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names) we know that protocol is the second column and service is the third (tag is the last one and not the first as appears in the page).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first, let's get the protocols.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icmp', 'tcp', 'udp']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data = [row.split(\",\") for row in raw_data]\n",
    "# TODO: generate a new RDD 'protocols' as a collection of *distinct* protocol names\n",
    "# HINT: protocol is the 2nd value of each row\n",
    "\n",
    "protocols = list(set([row[1] for row in csv_data]))\n",
    "protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for services.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['urh_i',\n",
       " 'hostnames',\n",
       " 'vmnet',\n",
       " 'ftp_data',\n",
       " 'gopher',\n",
       " 'IRC',\n",
       " 'daytime',\n",
       " 'smtp',\n",
       " 'printer',\n",
       " 'remote_job',\n",
       " 'sunrpc',\n",
       " 'link',\n",
       " 'bgp',\n",
       " 'nnsp',\n",
       " 'private',\n",
       " 'courier',\n",
       " 'rje',\n",
       " 'mtp',\n",
       " 'klogin',\n",
       " 'urp_i',\n",
       " 'http',\n",
       " 'ldap',\n",
       " 'csnet_ns',\n",
       " 'sql_net',\n",
       " 'systat',\n",
       " 'auth',\n",
       " 'iso_tsap',\n",
       " 'eco_i',\n",
       " 'finger',\n",
       " 'pop_3',\n",
       " 'uucp',\n",
       " 'pop_2',\n",
       " 'ecr_i',\n",
       " 'ftp',\n",
       " 'netbios_ssn',\n",
       " 'ntp_u',\n",
       " 'echo',\n",
       " 'domain_u',\n",
       " 'netbios_ns',\n",
       " 'login',\n",
       " 'netbios_dgm',\n",
       " 'time',\n",
       " 'discard',\n",
       " 'kshell',\n",
       " 'netstat',\n",
       " 'Z39_50',\n",
       " 'ssh',\n",
       " 'supdup',\n",
       " 'other',\n",
       " 'name',\n",
       " 'nntp',\n",
       " 'http_443',\n",
       " 'telnet',\n",
       " 'efs',\n",
       " 'uucp_path',\n",
       " 'exec',\n",
       " 'whois',\n",
       " 'imap4',\n",
       " 'ctf',\n",
       " 'domain',\n",
       " 'shell']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: generate a new RDD 'services' as a collection of *distinct* service names\n",
    "# HINT: protocol is the 3rd value of each row\n",
    "services = list(set([row[2] for row in csv_data]))\n",
    "print( len(services))\n",
    "services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A longer list in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the cartesian product.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 183 combinations of protocol X service\n"
     ]
    }
   ],
   "source": [
    "product = [(protocol, service) for protocol in protocols for service in services]\n",
    "print(\"There are {} combinations of protocol X service\".format(len(product)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, for such small RDDs doesn't really make sense to use Spark cartesian product. We could have perfectly collected the values after using `distinct` and do the cartesian product locally. Moreover, `distinct` and `cartesian` are expensive operations so they must be used with care when the operating datasets are large.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
